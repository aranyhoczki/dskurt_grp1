{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../datasets/train1.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>device_id</th>\n",
       "      <th>events_num</th>\n",
       "      <th>min_timestamp</th>\n",
       "      <th>max_timestamp</th>\n",
       "      <th>events_per_day</th>\n",
       "      <th>label_548</th>\n",
       "      <th>label_i_548</th>\n",
       "      <th>label_704</th>\n",
       "      <th>label_i_704</th>\n",
       "      <th>...</th>\n",
       "      <th>label_787</th>\n",
       "      <th>label_i_787</th>\n",
       "      <th>label_22222</th>\n",
       "      <th>label_i_22222</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>group</th>\n",
       "      <th>phone_brand</th>\n",
       "      <th>device_model</th>\n",
       "      <th>phone_brand_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-8260683887967679142</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-01 14:23:37</td>\n",
       "      <td>2016-05-01 14:23:37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>102</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>M32-38</td>\n",
       "      <td>小米</td>\n",
       "      <td>MI 2</td>\n",
       "      <td>Xiaomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7477216237379271436</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-05-04 14:08:17</td>\n",
       "      <td>2016-05-06 18:51:15</td>\n",
       "      <td>0.065502</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>78</td>\n",
       "      <td>F</td>\n",
       "      <td>37</td>\n",
       "      <td>F33-42</td>\n",
       "      <td>华为</td>\n",
       "      <td>荣耀6 plus</td>\n",
       "      <td>Huawei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6352067998666467520</td>\n",
       "      <td>11</td>\n",
       "      <td>2016-05-03 22:14:30</td>\n",
       "      <td>2016-05-05 17:27:58</td>\n",
       "      <td>0.072820</td>\n",
       "      <td>70</td>\n",
       "      <td>34</td>\n",
       "      <td>57</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>103</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>M32-38</td>\n",
       "      <td>华为</td>\n",
       "      <td>荣耀畅玩4X</td>\n",
       "      <td>Huawei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1508636020748379883</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-05-01 12:41:38</td>\n",
       "      <td>2016-05-05 12:34:15</td>\n",
       "      <td>0.198975</td>\n",
       "      <td>50</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>F</td>\n",
       "      <td>28</td>\n",
       "      <td>F27-28</td>\n",
       "      <td>华为</td>\n",
       "      <td>荣耀畅玩4X</td>\n",
       "      <td>Huawei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-6876541075223249434</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-05-01 02:51:41</td>\n",
       "      <td>2016-05-06 16:30:01</td>\n",
       "      <td>0.094715</td>\n",
       "      <td>63</td>\n",
       "      <td>187</td>\n",
       "      <td>46</td>\n",
       "      <td>162</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>520</td>\n",
       "      <td>M</td>\n",
       "      <td>75</td>\n",
       "      <td>M39+</td>\n",
       "      <td>魅族</td>\n",
       "      <td>魅蓝NOTE</td>\n",
       "      <td>Meizu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            device_id  events_num        min_timestamp  \\\n",
       "0           0 -8260683887967679142           1  2016-05-01 14:23:37   \n",
       "1           1  7477216237379271436           3  2016-05-04 14:08:17   \n",
       "2           2  6352067998666467520          11  2016-05-03 22:14:30   \n",
       "3           3  1508636020748379883           5  2016-05-01 12:41:38   \n",
       "4           4 -6876541075223249434           6  2016-05-01 02:51:41   \n",
       "\n",
       "         max_timestamp  events_per_day  label_548  label_i_548  label_704  \\\n",
       "0  2016-05-01 14:23:37        0.000000          5           30          3   \n",
       "1  2016-05-06 18:51:15        0.065502         31           29         25   \n",
       "2  2016-05-05 17:27:58        0.072820         70           34         57   \n",
       "3  2016-05-05 12:34:15        0.198975         50           39         38   \n",
       "4  2016-05-06 16:30:01        0.094715         63          187         46   \n",
       "\n",
       "   label_i_704       ...         label_787  label_i_787  label_22222  \\\n",
       "0           24       ...                 0            0            4   \n",
       "1           17       ...                 3            0           27   \n",
       "2           19       ...                11            0          211   \n",
       "3           36       ...                 2            3           69   \n",
       "4          162       ...                 0            0          126   \n",
       "\n",
       "   label_i_22222  gender  age   group  phone_brand  device_model  \\\n",
       "0            102       M   35  M32-38           小米          MI 2   \n",
       "1             78       F   37  F33-42           华为      荣耀6 plus   \n",
       "2            103       M   32  M32-38           华为        荣耀畅玩4X   \n",
       "3             80       F   28  F27-28           华为        荣耀畅玩4X   \n",
       "4            520       M   75    M39+           魅族        魅蓝NOTE   \n",
       "\n",
       "   phone_brand_eng  \n",
       "0           Xiaomi  \n",
       "1           Huawei  \n",
       "2           Huawei  \n",
       "3           Huawei  \n",
       "4            Meizu  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   num_boost_round = 500\n",
    "    early_stopping_rounds = 20\n",
    "    test_size = 0.2\n",
    "\n",
    "    X_train, X_valid = train_test_split(train, test_size=test_size, random_state=random_state)\n",
    "    print('Length train:', len(X_train.index))\n",
    "    print('Length valid:', len(X_valid.index))\n",
    "    y_train = X_train[target]\n",
    "    y_valid = X_valid[target]\n",
    "    dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "\n",
    "    print(\"Validating...\")\n",
    "    check = gbm.predict(xgb.DMatrix(X_valid[features]), ntree_limit=gbm.best_iteration)\n",
    "    score = log_loss(y_valid.tolist(), check)\n",
    "\n",
    "    imp = get_importance(gbm, features)\n",
    "    print('Importance array: ', imp)\n",
    "\n",
    "    print(\"Predict test set...\")\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test[features]), ntree_limit=gbm.best_iteration)\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return test_prediction.tolist(), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = pd.factorize(df_train['phone_brand_eng'])\n",
    "df_train.phone_brand_eng = factor[0]\n",
    "definitions_phone_brand_eng = factor[1]\n",
    "factor = pd.factorize(df_train['device_model'])\n",
    "df_train.device_model = factor[0]\n",
    "definitions_device_model = factor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = pd.factorize(df_train['group'])\n",
    "df_train.group = factor[0]\n",
    "definitions = factor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "magyarazo_valtozok = []\n",
    "for m in df_train.columns:\n",
    "    if m not in ['device_id', 'min_timestamp', 'max_timestamp','gender', 'age', 'group', 'phone_brand' ]:\n",
    "        magyarazo_valtozok += [m]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[magyarazo_valtozok], df_train['group'], test_size=0.2,random_state=109) # 90% training and 10% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akos/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>device_id</th>\n",
       "      <th>events_num</th>\n",
       "      <th>min_timestamp</th>\n",
       "      <th>max_timestamp</th>\n",
       "      <th>events_per_day</th>\n",
       "      <th>label_548</th>\n",
       "      <th>label_i_548</th>\n",
       "      <th>label_704</th>\n",
       "      <th>label_i_704</th>\n",
       "      <th>...</th>\n",
       "      <th>label_787</th>\n",
       "      <th>label_i_787</th>\n",
       "      <th>label_22222</th>\n",
       "      <th>label_i_22222</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>group</th>\n",
       "      <th>phone_brand</th>\n",
       "      <th>device_model</th>\n",
       "      <th>phone_brand_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-8260683887967679142</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-01 14:23:37</td>\n",
       "      <td>2016-05-01 14:23:37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>102</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>小米</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7477216237379271436</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-05-04 14:08:17</td>\n",
       "      <td>2016-05-06 18:51:15</td>\n",
       "      <td>0.065502</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>78</td>\n",
       "      <td>F</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>华为</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6352067998666467520</td>\n",
       "      <td>11</td>\n",
       "      <td>2016-05-03 22:14:30</td>\n",
       "      <td>2016-05-05 17:27:58</td>\n",
       "      <td>0.072820</td>\n",
       "      <td>70</td>\n",
       "      <td>34</td>\n",
       "      <td>57</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>103</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>华为</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1508636020748379883</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-05-01 12:41:38</td>\n",
       "      <td>2016-05-05 12:34:15</td>\n",
       "      <td>0.198975</td>\n",
       "      <td>50</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>F</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>华为</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-6876541075223249434</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-05-01 02:51:41</td>\n",
       "      <td>2016-05-06 16:30:01</td>\n",
       "      <td>0.094715</td>\n",
       "      <td>63</td>\n",
       "      <td>187</td>\n",
       "      <td>46</td>\n",
       "      <td>162</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>520</td>\n",
       "      <td>M</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>魅族</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            device_id  events_num        min_timestamp  \\\n",
       "0           0 -8260683887967679142           1  2016-05-01 14:23:37   \n",
       "1           1  7477216237379271436           3  2016-05-04 14:08:17   \n",
       "2           2  6352067998666467520          11  2016-05-03 22:14:30   \n",
       "3           3  1508636020748379883           5  2016-05-01 12:41:38   \n",
       "4           4 -6876541075223249434           6  2016-05-01 02:51:41   \n",
       "\n",
       "         max_timestamp  events_per_day  label_548  label_i_548  label_704  \\\n",
       "0  2016-05-01 14:23:37        0.000000          5           30          3   \n",
       "1  2016-05-06 18:51:15        0.065502         31           29         25   \n",
       "2  2016-05-05 17:27:58        0.072820         70           34         57   \n",
       "3  2016-05-05 12:34:15        0.198975         50           39         38   \n",
       "4  2016-05-06 16:30:01        0.094715         63          187         46   \n",
       "\n",
       "   label_i_704       ...         label_787  label_i_787  label_22222  \\\n",
       "0           24       ...                 0            0            4   \n",
       "1           17       ...                 3            0           27   \n",
       "2           19       ...                11            0          211   \n",
       "3           36       ...                 2            3           69   \n",
       "4          162       ...                 0            0          126   \n",
       "\n",
       "   label_i_22222  gender  age  group  phone_brand  device_model  \\\n",
       "0            102       M   35      0           小米             0   \n",
       "1             78       F   37      1           华为             1   \n",
       "2            103       M   32      0           华为             2   \n",
       "3             80       F   28      2           华为             2   \n",
       "4            520       M   75      3           魅族             3   \n",
       "\n",
       "   phone_brand_eng  \n",
       "0                0  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                2  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb parameterek ... nem tudom, hogy mi hogy számít...\n",
    "\n",
    "eta = 0.025\n",
    "max_depth = 7\n",
    "subsample = 0.75\n",
    "colsample_bytree = 0.75\n",
    "random_state=123\n",
    "params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": 12,\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"eta\": eta,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": random_state,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.47338\teval-mlogloss:2.4791\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-mlogloss:2.46178\teval-mlogloss:2.47377\n",
      "[2]\ttrain-mlogloss:2.45032\teval-mlogloss:2.46884\n",
      "[3]\ttrain-mlogloss:2.43894\teval-mlogloss:2.46355\n",
      "[4]\ttrain-mlogloss:2.42825\teval-mlogloss:2.4587\n",
      "[5]\ttrain-mlogloss:2.41748\teval-mlogloss:2.45391\n",
      "[6]\ttrain-mlogloss:2.40684\teval-mlogloss:2.44905\n",
      "[7]\ttrain-mlogloss:2.39682\teval-mlogloss:2.44446\n",
      "[8]\ttrain-mlogloss:2.38673\teval-mlogloss:2.44013\n",
      "[9]\ttrain-mlogloss:2.37671\teval-mlogloss:2.43599\n",
      "[10]\ttrain-mlogloss:2.36676\teval-mlogloss:2.43198\n",
      "[11]\ttrain-mlogloss:2.3571\teval-mlogloss:2.42803\n",
      "[12]\ttrain-mlogloss:2.34761\teval-mlogloss:2.42452\n",
      "[13]\ttrain-mlogloss:2.33796\teval-mlogloss:2.42079\n",
      "[14]\ttrain-mlogloss:2.3288\teval-mlogloss:2.41732\n",
      "[15]\ttrain-mlogloss:2.31986\teval-mlogloss:2.41383\n",
      "[16]\ttrain-mlogloss:2.31066\teval-mlogloss:2.41048\n",
      "[17]\ttrain-mlogloss:2.3019\teval-mlogloss:2.40724\n",
      "[18]\ttrain-mlogloss:2.29339\teval-mlogloss:2.40423\n",
      "[19]\ttrain-mlogloss:2.28511\teval-mlogloss:2.40125\n",
      "[20]\ttrain-mlogloss:2.2767\teval-mlogloss:2.39836\n",
      "[21]\ttrain-mlogloss:2.26854\teval-mlogloss:2.39522\n",
      "[22]\ttrain-mlogloss:2.26032\teval-mlogloss:2.39249\n",
      "[23]\ttrain-mlogloss:2.25281\teval-mlogloss:2.38971\n",
      "[24]\ttrain-mlogloss:2.24504\teval-mlogloss:2.38686\n",
      "[25]\ttrain-mlogloss:2.23726\teval-mlogloss:2.3841\n",
      "[26]\ttrain-mlogloss:2.22948\teval-mlogloss:2.38154\n",
      "[27]\ttrain-mlogloss:2.22181\teval-mlogloss:2.37894\n",
      "[28]\ttrain-mlogloss:2.21461\teval-mlogloss:2.37648\n",
      "[29]\ttrain-mlogloss:2.20709\teval-mlogloss:2.37389\n",
      "[30]\ttrain-mlogloss:2.1998\teval-mlogloss:2.37149\n",
      "[31]\ttrain-mlogloss:2.19244\teval-mlogloss:2.36911\n",
      "[32]\ttrain-mlogloss:2.18547\teval-mlogloss:2.36693\n",
      "[33]\ttrain-mlogloss:2.178\teval-mlogloss:2.36497\n",
      "[34]\ttrain-mlogloss:2.17096\teval-mlogloss:2.36279\n",
      "[35]\ttrain-mlogloss:2.16394\teval-mlogloss:2.3607\n",
      "[36]\ttrain-mlogloss:2.15708\teval-mlogloss:2.35863\n",
      "[37]\ttrain-mlogloss:2.15045\teval-mlogloss:2.3567\n",
      "[38]\ttrain-mlogloss:2.14353\teval-mlogloss:2.35476\n",
      "[39]\ttrain-mlogloss:2.1368\teval-mlogloss:2.35276\n",
      "[40]\ttrain-mlogloss:2.13047\teval-mlogloss:2.35079\n",
      "[41]\ttrain-mlogloss:2.12386\teval-mlogloss:2.34906\n",
      "[42]\ttrain-mlogloss:2.11737\teval-mlogloss:2.34746\n",
      "[43]\ttrain-mlogloss:2.11131\teval-mlogloss:2.34556\n",
      "[44]\ttrain-mlogloss:2.10497\teval-mlogloss:2.34397\n",
      "[45]\ttrain-mlogloss:2.09884\teval-mlogloss:2.34211\n",
      "[46]\ttrain-mlogloss:2.09298\teval-mlogloss:2.34075\n",
      "[47]\ttrain-mlogloss:2.08694\teval-mlogloss:2.3391\n",
      "[48]\ttrain-mlogloss:2.08094\teval-mlogloss:2.33771\n",
      "[49]\ttrain-mlogloss:2.0748\teval-mlogloss:2.33626\n",
      "[50]\ttrain-mlogloss:2.06906\teval-mlogloss:2.33465\n",
      "[51]\ttrain-mlogloss:2.0632\teval-mlogloss:2.33313\n",
      "[52]\ttrain-mlogloss:2.05721\teval-mlogloss:2.33171\n",
      "[53]\ttrain-mlogloss:2.05157\teval-mlogloss:2.33051\n",
      "[54]\ttrain-mlogloss:2.04579\teval-mlogloss:2.32918\n",
      "[55]\ttrain-mlogloss:2.04044\teval-mlogloss:2.32797\n",
      "[56]\ttrain-mlogloss:2.03472\teval-mlogloss:2.32677\n",
      "[57]\ttrain-mlogloss:2.02913\teval-mlogloss:2.32546\n",
      "[58]\ttrain-mlogloss:2.0238\teval-mlogloss:2.32424\n",
      "[59]\ttrain-mlogloss:2.0187\teval-mlogloss:2.3231\n",
      "[60]\ttrain-mlogloss:2.01367\teval-mlogloss:2.32187\n",
      "[61]\ttrain-mlogloss:2.00843\teval-mlogloss:2.32067\n",
      "[62]\ttrain-mlogloss:2.00312\teval-mlogloss:2.31944\n",
      "[63]\ttrain-mlogloss:1.99818\teval-mlogloss:2.31818\n",
      "[64]\ttrain-mlogloss:1.99291\teval-mlogloss:2.31708\n",
      "[65]\ttrain-mlogloss:1.98765\teval-mlogloss:2.316\n",
      "[66]\ttrain-mlogloss:1.98246\teval-mlogloss:2.31508\n",
      "[67]\ttrain-mlogloss:1.97754\teval-mlogloss:2.31432\n",
      "[68]\ttrain-mlogloss:1.97253\teval-mlogloss:2.31334\n",
      "[69]\ttrain-mlogloss:1.96752\teval-mlogloss:2.31232\n",
      "[70]\ttrain-mlogloss:1.96281\teval-mlogloss:2.31157\n",
      "[71]\ttrain-mlogloss:1.95822\teval-mlogloss:2.31063\n",
      "[72]\ttrain-mlogloss:1.95353\teval-mlogloss:2.30954\n",
      "[73]\ttrain-mlogloss:1.94869\teval-mlogloss:2.30856\n",
      "[74]\ttrain-mlogloss:1.9442\teval-mlogloss:2.30775\n",
      "[75]\ttrain-mlogloss:1.93955\teval-mlogloss:2.30701\n",
      "[76]\ttrain-mlogloss:1.93458\teval-mlogloss:2.30602\n",
      "[77]\ttrain-mlogloss:1.92973\teval-mlogloss:2.30503\n",
      "[78]\ttrain-mlogloss:1.92518\teval-mlogloss:2.30402\n",
      "[79]\ttrain-mlogloss:1.92035\teval-mlogloss:2.30327\n",
      "[80]\ttrain-mlogloss:1.91558\teval-mlogloss:2.30231\n",
      "[81]\ttrain-mlogloss:1.91106\teval-mlogloss:2.30142\n",
      "[82]\ttrain-mlogloss:1.9064\teval-mlogloss:2.3007\n",
      "[83]\ttrain-mlogloss:1.90214\teval-mlogloss:2.29972\n",
      "[84]\ttrain-mlogloss:1.8979\teval-mlogloss:2.29885\n",
      "[85]\ttrain-mlogloss:1.89342\teval-mlogloss:2.29811\n",
      "[86]\ttrain-mlogloss:1.88905\teval-mlogloss:2.29746\n",
      "[87]\ttrain-mlogloss:1.88481\teval-mlogloss:2.29663\n",
      "[88]\ttrain-mlogloss:1.8802\teval-mlogloss:2.29595\n",
      "[89]\ttrain-mlogloss:1.87618\teval-mlogloss:2.29537\n",
      "[90]\ttrain-mlogloss:1.87177\teval-mlogloss:2.29463\n",
      "[91]\ttrain-mlogloss:1.8676\teval-mlogloss:2.29384\n",
      "[92]\ttrain-mlogloss:1.86356\teval-mlogloss:2.2932\n",
      "[93]\ttrain-mlogloss:1.8594\teval-mlogloss:2.29248\n",
      "[94]\ttrain-mlogloss:1.85506\teval-mlogloss:2.29187\n",
      "[95]\ttrain-mlogloss:1.85111\teval-mlogloss:2.29122\n",
      "[96]\ttrain-mlogloss:1.84689\teval-mlogloss:2.29066\n",
      "[97]\ttrain-mlogloss:1.84288\teval-mlogloss:2.29015\n",
      "[98]\ttrain-mlogloss:1.83889\teval-mlogloss:2.28952\n",
      "[99]\ttrain-mlogloss:1.835\teval-mlogloss:2.28898\n",
      "[100]\ttrain-mlogloss:1.83108\teval-mlogloss:2.28843\n",
      "[101]\ttrain-mlogloss:1.82724\teval-mlogloss:2.28798\n",
      "[102]\ttrain-mlogloss:1.82327\teval-mlogloss:2.28742\n",
      "[103]\ttrain-mlogloss:1.81923\teval-mlogloss:2.28699\n",
      "[104]\ttrain-mlogloss:1.81516\teval-mlogloss:2.28663\n",
      "[105]\ttrain-mlogloss:1.81154\teval-mlogloss:2.28622\n",
      "[106]\ttrain-mlogloss:1.80727\teval-mlogloss:2.28573\n",
      "[107]\ttrain-mlogloss:1.80324\teval-mlogloss:2.28539\n",
      "[108]\ttrain-mlogloss:1.79963\teval-mlogloss:2.28491\n",
      "[109]\ttrain-mlogloss:1.79593\teval-mlogloss:2.28447\n",
      "[110]\ttrain-mlogloss:1.79218\teval-mlogloss:2.28406\n",
      "[111]\ttrain-mlogloss:1.78815\teval-mlogloss:2.28367\n",
      "[112]\ttrain-mlogloss:1.78425\teval-mlogloss:2.28328\n",
      "[113]\ttrain-mlogloss:1.78079\teval-mlogloss:2.2829\n",
      "[114]\ttrain-mlogloss:1.77696\teval-mlogloss:2.28229\n",
      "[115]\ttrain-mlogloss:1.77348\teval-mlogloss:2.28192\n",
      "[116]\ttrain-mlogloss:1.76967\teval-mlogloss:2.2814\n",
      "[117]\ttrain-mlogloss:1.76599\teval-mlogloss:2.28096\n",
      "[118]\ttrain-mlogloss:1.76225\teval-mlogloss:2.28064\n",
      "[119]\ttrain-mlogloss:1.75867\teval-mlogloss:2.28023\n",
      "[120]\ttrain-mlogloss:1.75538\teval-mlogloss:2.27992\n",
      "[121]\ttrain-mlogloss:1.75173\teval-mlogloss:2.27978\n",
      "[122]\ttrain-mlogloss:1.74824\teval-mlogloss:2.27941\n",
      "[123]\ttrain-mlogloss:1.7447\teval-mlogloss:2.27896\n",
      "[124]\ttrain-mlogloss:1.74118\teval-mlogloss:2.27868\n",
      "[125]\ttrain-mlogloss:1.73782\teval-mlogloss:2.27843\n",
      "[126]\ttrain-mlogloss:1.73429\teval-mlogloss:2.27807\n",
      "[127]\ttrain-mlogloss:1.73104\teval-mlogloss:2.27763\n",
      "[128]\ttrain-mlogloss:1.72785\teval-mlogloss:2.27735\n",
      "[129]\ttrain-mlogloss:1.72466\teval-mlogloss:2.27675\n",
      "[130]\ttrain-mlogloss:1.72107\teval-mlogloss:2.27645\n",
      "[131]\ttrain-mlogloss:1.71785\teval-mlogloss:2.27608\n",
      "[132]\ttrain-mlogloss:1.71427\teval-mlogloss:2.27553\n",
      "[133]\ttrain-mlogloss:1.71092\teval-mlogloss:2.27523\n",
      "[134]\ttrain-mlogloss:1.70785\teval-mlogloss:2.27489\n",
      "[135]\ttrain-mlogloss:1.70448\teval-mlogloss:2.27467\n",
      "[136]\ttrain-mlogloss:1.70104\teval-mlogloss:2.27439\n",
      "[137]\ttrain-mlogloss:1.69772\teval-mlogloss:2.27411\n",
      "[138]\ttrain-mlogloss:1.69477\teval-mlogloss:2.27378\n",
      "[139]\ttrain-mlogloss:1.69132\teval-mlogloss:2.27376\n",
      "[140]\ttrain-mlogloss:1.68836\teval-mlogloss:2.27352\n",
      "[141]\ttrain-mlogloss:1.68476\teval-mlogloss:2.27319\n",
      "[142]\ttrain-mlogloss:1.68139\teval-mlogloss:2.27282\n",
      "[143]\ttrain-mlogloss:1.67854\teval-mlogloss:2.27251\n",
      "[144]\ttrain-mlogloss:1.67539\teval-mlogloss:2.2723\n",
      "[145]\ttrain-mlogloss:1.67211\teval-mlogloss:2.27228\n",
      "[146]\ttrain-mlogloss:1.66884\teval-mlogloss:2.27192\n",
      "[147]\ttrain-mlogloss:1.66512\teval-mlogloss:2.27183\n",
      "[148]\ttrain-mlogloss:1.66206\teval-mlogloss:2.27151\n",
      "[149]\ttrain-mlogloss:1.65916\teval-mlogloss:2.27131\n",
      "[150]\ttrain-mlogloss:1.65585\teval-mlogloss:2.27125\n",
      "[151]\ttrain-mlogloss:1.6525\teval-mlogloss:2.27105\n",
      "[152]\ttrain-mlogloss:1.64943\teval-mlogloss:2.27084\n",
      "[153]\ttrain-mlogloss:1.64649\teval-mlogloss:2.27058\n",
      "[154]\ttrain-mlogloss:1.64377\teval-mlogloss:2.27054\n",
      "[155]\ttrain-mlogloss:1.64086\teval-mlogloss:2.27043\n",
      "[156]\ttrain-mlogloss:1.63807\teval-mlogloss:2.27026\n",
      "[157]\ttrain-mlogloss:1.63483\teval-mlogloss:2.27001\n",
      "[158]\ttrain-mlogloss:1.63178\teval-mlogloss:2.26987\n",
      "[159]\ttrain-mlogloss:1.62896\teval-mlogloss:2.26979\n",
      "[160]\ttrain-mlogloss:1.62593\teval-mlogloss:2.26963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-mlogloss:1.62263\teval-mlogloss:2.26932\n",
      "[162]\ttrain-mlogloss:1.61997\teval-mlogloss:2.26917\n",
      "[163]\ttrain-mlogloss:1.61709\teval-mlogloss:2.26879\n",
      "[164]\ttrain-mlogloss:1.61395\teval-mlogloss:2.26849\n",
      "[165]\ttrain-mlogloss:1.61072\teval-mlogloss:2.26836\n",
      "[166]\ttrain-mlogloss:1.60758\teval-mlogloss:2.26818\n",
      "[167]\ttrain-mlogloss:1.60504\teval-mlogloss:2.26808\n",
      "[168]\ttrain-mlogloss:1.6024\teval-mlogloss:2.26801\n",
      "[169]\ttrain-mlogloss:1.59954\teval-mlogloss:2.26786\n",
      "[170]\ttrain-mlogloss:1.59698\teval-mlogloss:2.26774\n",
      "[171]\ttrain-mlogloss:1.59438\teval-mlogloss:2.26763\n",
      "[172]\ttrain-mlogloss:1.59151\teval-mlogloss:2.26746\n",
      "[173]\ttrain-mlogloss:1.58841\teval-mlogloss:2.26723\n",
      "[174]\ttrain-mlogloss:1.58577\teval-mlogloss:2.26698\n",
      "[175]\ttrain-mlogloss:1.58339\teval-mlogloss:2.26675\n",
      "[176]\ttrain-mlogloss:1.58077\teval-mlogloss:2.26663\n",
      "[177]\ttrain-mlogloss:1.57793\teval-mlogloss:2.26633\n",
      "[178]\ttrain-mlogloss:1.57524\teval-mlogloss:2.26625\n",
      "[179]\ttrain-mlogloss:1.57243\teval-mlogloss:2.26606\n",
      "[180]\ttrain-mlogloss:1.56966\teval-mlogloss:2.26587\n",
      "[181]\ttrain-mlogloss:1.56683\teval-mlogloss:2.26574\n",
      "[182]\ttrain-mlogloss:1.5642\teval-mlogloss:2.26564\n",
      "[183]\ttrain-mlogloss:1.56104\teval-mlogloss:2.26555\n",
      "[184]\ttrain-mlogloss:1.55853\teval-mlogloss:2.26537\n",
      "[185]\ttrain-mlogloss:1.55578\teval-mlogloss:2.26505\n",
      "[186]\ttrain-mlogloss:1.55304\teval-mlogloss:2.26485\n",
      "[187]\ttrain-mlogloss:1.55027\teval-mlogloss:2.26471\n",
      "[188]\ttrain-mlogloss:1.54787\teval-mlogloss:2.26466\n",
      "[189]\ttrain-mlogloss:1.54514\teval-mlogloss:2.26459\n",
      "[190]\ttrain-mlogloss:1.54235\teval-mlogloss:2.26437\n",
      "[191]\ttrain-mlogloss:1.53953\teval-mlogloss:2.26424\n",
      "[192]\ttrain-mlogloss:1.53683\teval-mlogloss:2.26412\n",
      "[193]\ttrain-mlogloss:1.53406\teval-mlogloss:2.26396\n",
      "[194]\ttrain-mlogloss:1.53142\teval-mlogloss:2.2638\n",
      "[195]\ttrain-mlogloss:1.52866\teval-mlogloss:2.26378\n",
      "[196]\ttrain-mlogloss:1.52591\teval-mlogloss:2.26373\n",
      "[197]\ttrain-mlogloss:1.52323\teval-mlogloss:2.26359\n",
      "[198]\ttrain-mlogloss:1.52078\teval-mlogloss:2.2635\n",
      "[199]\ttrain-mlogloss:1.51862\teval-mlogloss:2.2633\n",
      "[200]\ttrain-mlogloss:1.51623\teval-mlogloss:2.26313\n",
      "[201]\ttrain-mlogloss:1.51423\teval-mlogloss:2.26308\n",
      "[202]\ttrain-mlogloss:1.51171\teval-mlogloss:2.26305\n",
      "[203]\ttrain-mlogloss:1.509\teval-mlogloss:2.26294\n",
      "[204]\ttrain-mlogloss:1.50651\teval-mlogloss:2.26295\n",
      "[205]\ttrain-mlogloss:1.50391\teval-mlogloss:2.26299\n",
      "[206]\ttrain-mlogloss:1.50171\teval-mlogloss:2.26284\n",
      "[207]\ttrain-mlogloss:1.49906\teval-mlogloss:2.26268\n",
      "[208]\ttrain-mlogloss:1.49674\teval-mlogloss:2.26251\n",
      "[209]\ttrain-mlogloss:1.49471\teval-mlogloss:2.26239\n",
      "[210]\ttrain-mlogloss:1.49196\teval-mlogloss:2.26241\n",
      "[211]\ttrain-mlogloss:1.48947\teval-mlogloss:2.26221\n",
      "[212]\ttrain-mlogloss:1.48739\teval-mlogloss:2.26209\n",
      "[213]\ttrain-mlogloss:1.48497\teval-mlogloss:2.26208\n",
      "[214]\ttrain-mlogloss:1.48226\teval-mlogloss:2.26192\n",
      "[215]\ttrain-mlogloss:1.48007\teval-mlogloss:2.26179\n",
      "[216]\ttrain-mlogloss:1.47741\teval-mlogloss:2.2616\n",
      "[217]\ttrain-mlogloss:1.47498\teval-mlogloss:2.26158\n",
      "[218]\ttrain-mlogloss:1.47295\teval-mlogloss:2.26155\n",
      "[219]\ttrain-mlogloss:1.47033\teval-mlogloss:2.26136\n",
      "[220]\ttrain-mlogloss:1.46779\teval-mlogloss:2.26126\n",
      "[221]\ttrain-mlogloss:1.46503\teval-mlogloss:2.26111\n",
      "[222]\ttrain-mlogloss:1.46256\teval-mlogloss:2.26105\n",
      "[223]\ttrain-mlogloss:1.45998\teval-mlogloss:2.26089\n",
      "[224]\ttrain-mlogloss:1.45769\teval-mlogloss:2.26094\n",
      "[225]\ttrain-mlogloss:1.45534\teval-mlogloss:2.26104\n",
      "[226]\ttrain-mlogloss:1.45303\teval-mlogloss:2.26097\n",
      "[227]\ttrain-mlogloss:1.45054\teval-mlogloss:2.26103\n",
      "[228]\ttrain-mlogloss:1.44827\teval-mlogloss:2.261\n",
      "[229]\ttrain-mlogloss:1.44591\teval-mlogloss:2.26101\n",
      "[230]\ttrain-mlogloss:1.4437\teval-mlogloss:2.261\n",
      "[231]\ttrain-mlogloss:1.44074\teval-mlogloss:2.261\n",
      "[232]\ttrain-mlogloss:1.43825\teval-mlogloss:2.26101\n",
      "[233]\ttrain-mlogloss:1.43583\teval-mlogloss:2.26089\n",
      "[234]\ttrain-mlogloss:1.43327\teval-mlogloss:2.26083\n",
      "[235]\ttrain-mlogloss:1.43095\teval-mlogloss:2.26079\n",
      "[236]\ttrain-mlogloss:1.4288\teval-mlogloss:2.26069\n",
      "[237]\ttrain-mlogloss:1.42664\teval-mlogloss:2.26048\n",
      "[238]\ttrain-mlogloss:1.42434\teval-mlogloss:2.26037\n",
      "[239]\ttrain-mlogloss:1.42182\teval-mlogloss:2.2604\n",
      "[240]\ttrain-mlogloss:1.41936\teval-mlogloss:2.26032\n",
      "[241]\ttrain-mlogloss:1.41705\teval-mlogloss:2.26034\n",
      "[242]\ttrain-mlogloss:1.41482\teval-mlogloss:2.26037\n",
      "[243]\ttrain-mlogloss:1.4125\teval-mlogloss:2.26036\n",
      "[244]\ttrain-mlogloss:1.41036\teval-mlogloss:2.26032\n",
      "[245]\ttrain-mlogloss:1.40843\teval-mlogloss:2.26033\n",
      "[246]\ttrain-mlogloss:1.40614\teval-mlogloss:2.26025\n",
      "[247]\ttrain-mlogloss:1.40407\teval-mlogloss:2.26019\n",
      "[248]\ttrain-mlogloss:1.40183\teval-mlogloss:2.26021\n",
      "[249]\ttrain-mlogloss:1.39961\teval-mlogloss:2.2601\n",
      "[250]\ttrain-mlogloss:1.39723\teval-mlogloss:2.25995\n",
      "[251]\ttrain-mlogloss:1.39541\teval-mlogloss:2.25984\n",
      "[252]\ttrain-mlogloss:1.39312\teval-mlogloss:2.25976\n",
      "[253]\ttrain-mlogloss:1.39089\teval-mlogloss:2.25977\n",
      "[254]\ttrain-mlogloss:1.38855\teval-mlogloss:2.25984\n",
      "[255]\ttrain-mlogloss:1.38654\teval-mlogloss:2.25983\n",
      "[256]\ttrain-mlogloss:1.38441\teval-mlogloss:2.25969\n",
      "[257]\ttrain-mlogloss:1.38236\teval-mlogloss:2.25967\n",
      "[258]\ttrain-mlogloss:1.38006\teval-mlogloss:2.25955\n",
      "[259]\ttrain-mlogloss:1.37774\teval-mlogloss:2.25966\n",
      "[260]\ttrain-mlogloss:1.37593\teval-mlogloss:2.25955\n",
      "[261]\ttrain-mlogloss:1.37362\teval-mlogloss:2.25959\n",
      "[262]\ttrain-mlogloss:1.37129\teval-mlogloss:2.2595\n",
      "[263]\ttrain-mlogloss:1.36887\teval-mlogloss:2.25949\n",
      "[264]\ttrain-mlogloss:1.3668\teval-mlogloss:2.25944\n",
      "[265]\ttrain-mlogloss:1.36443\teval-mlogloss:2.25929\n",
      "[266]\ttrain-mlogloss:1.36252\teval-mlogloss:2.25927\n",
      "[267]\ttrain-mlogloss:1.36035\teval-mlogloss:2.25931\n",
      "[268]\ttrain-mlogloss:1.35816\teval-mlogloss:2.25941\n",
      "[269]\ttrain-mlogloss:1.35592\teval-mlogloss:2.25927\n",
      "[270]\ttrain-mlogloss:1.35377\teval-mlogloss:2.25926\n",
      "[271]\ttrain-mlogloss:1.35143\teval-mlogloss:2.25917\n",
      "[272]\ttrain-mlogloss:1.34918\teval-mlogloss:2.25914\n",
      "[273]\ttrain-mlogloss:1.34705\teval-mlogloss:2.259\n",
      "[274]\ttrain-mlogloss:1.34516\teval-mlogloss:2.25892\n",
      "[275]\ttrain-mlogloss:1.34312\teval-mlogloss:2.25891\n",
      "[276]\ttrain-mlogloss:1.3409\teval-mlogloss:2.25888\n",
      "[277]\ttrain-mlogloss:1.33902\teval-mlogloss:2.25892\n",
      "[278]\ttrain-mlogloss:1.33669\teval-mlogloss:2.25898\n",
      "[279]\ttrain-mlogloss:1.33433\teval-mlogloss:2.25897\n",
      "[280]\ttrain-mlogloss:1.33223\teval-mlogloss:2.25896\n",
      "[281]\ttrain-mlogloss:1.33031\teval-mlogloss:2.25883\n",
      "[282]\ttrain-mlogloss:1.32841\teval-mlogloss:2.25894\n",
      "[283]\ttrain-mlogloss:1.32631\teval-mlogloss:2.25875\n",
      "[284]\ttrain-mlogloss:1.32427\teval-mlogloss:2.25881\n",
      "[285]\ttrain-mlogloss:1.32194\teval-mlogloss:2.25882\n",
      "[286]\ttrain-mlogloss:1.31973\teval-mlogloss:2.25867\n",
      "[287]\ttrain-mlogloss:1.31757\teval-mlogloss:2.25865\n",
      "[288]\ttrain-mlogloss:1.31568\teval-mlogloss:2.25876\n",
      "[289]\ttrain-mlogloss:1.31368\teval-mlogloss:2.25862\n",
      "[290]\ttrain-mlogloss:1.31164\teval-mlogloss:2.25874\n",
      "[291]\ttrain-mlogloss:1.30951\teval-mlogloss:2.25873\n",
      "[292]\ttrain-mlogloss:1.30795\teval-mlogloss:2.25876\n",
      "[293]\ttrain-mlogloss:1.30585\teval-mlogloss:2.25874\n",
      "[294]\ttrain-mlogloss:1.30363\teval-mlogloss:2.2587\n",
      "[295]\ttrain-mlogloss:1.30161\teval-mlogloss:2.25875\n",
      "[296]\ttrain-mlogloss:1.29943\teval-mlogloss:2.2587\n",
      "[297]\ttrain-mlogloss:1.29763\teval-mlogloss:2.25876\n",
      "[298]\ttrain-mlogloss:1.29564\teval-mlogloss:2.25873\n",
      "[299]\ttrain-mlogloss:1.29404\teval-mlogloss:2.25869\n",
      "[300]\ttrain-mlogloss:1.29213\teval-mlogloss:2.25865\n",
      "[301]\ttrain-mlogloss:1.28996\teval-mlogloss:2.25874\n",
      "[302]\ttrain-mlogloss:1.28821\teval-mlogloss:2.25865\n",
      "[303]\ttrain-mlogloss:1.28634\teval-mlogloss:2.2588\n",
      "[304]\ttrain-mlogloss:1.28424\teval-mlogloss:2.25882\n",
      "[305]\ttrain-mlogloss:1.28217\teval-mlogloss:2.25884\n",
      "[306]\ttrain-mlogloss:1.27997\teval-mlogloss:2.25881\n",
      "[307]\ttrain-mlogloss:1.27826\teval-mlogloss:2.25888\n",
      "[308]\ttrain-mlogloss:1.27624\teval-mlogloss:2.25884\n",
      "[309]\ttrain-mlogloss:1.27432\teval-mlogloss:2.25887\n",
      "Stopping. Best iteration:\n",
      "[289]\ttrain-mlogloss:1.31368\teval-mlogloss:2.25862\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_boost_round = 500\n",
    "early_stopping_rounds = 20\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = gbm.predict(xgb.DMatrix(X_test), ntree_limit=gbm.best_iteration)\n",
    "score = log_loss(y_test.tolist(), check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.258756904724317"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14473328, 0.064509  , 0.0628477 , ..., 0.04197384, 0.0294369 ,\n",
       "        0.07602328],\n",
       "       [0.10571565, 0.1208485 , 0.0776386 , ..., 0.18730623, 0.0681701 ,\n",
       "        0.07533727],\n",
       "       [0.16422053, 0.06890288, 0.02246288, ..., 0.028362  , 0.02287088,\n",
       "        0.03618677],\n",
       "       ...,\n",
       "       [0.15586364, 0.12090624, 0.02196492, ..., 0.01817049, 0.01795727,\n",
       "        0.04790442],\n",
       "       [0.06252811, 0.03920196, 0.03509687, ..., 0.09249675, 0.13463558,\n",
       "        0.03805688],\n",
       "       [0.07680686, 0.05320404, 0.03988156, ..., 0.15965736, 0.18987392,\n",
       "        0.07866302]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
